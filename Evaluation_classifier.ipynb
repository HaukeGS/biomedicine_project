{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization & Setup"
      ],
      "metadata": {
        "id": "xtcgHa5COAs_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "J-nJiaMqar7p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch import nn, optim\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2YIikC3b3rb",
        "outputId": "f2730826-9e2c-4482-a487-a39e52caf6c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IGJ2xusxekd",
        "outputId": "62aaa64a-0254-407c-b568-9e1656c7e445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device is: cuda\n",
            "\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device is: {device}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "du_NFI00cDnx"
      },
      "outputs": [],
      "source": [
        "data_path = '/content/drive/MyDrive/biomedicine/embedded_data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn6-uMo-vNY1"
      },
      "source": [
        "# Hyper- and Controlparameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "k0yW-SUWzh1m"
      },
      "outputs": [],
      "source": [
        "# Controlparameter\n",
        "\n",
        "LOAD_FROM_DRIVE = False\n",
        "SAVE_TO_DRIVE = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "ZpPHuDWPvGIP"
      },
      "outputs": [],
      "source": [
        "# HYPERPARAMETER\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.0001\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu8pwzyWvQAt"
      },
      "source": [
        "# Custom Dataset & Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "erJSlcIOrgTz"
      },
      "outputs": [],
      "source": [
        "label_map = {\n",
        "    'control': 0,\n",
        "    'mild/moderate': 1,\n",
        "    'severe/critical': 2,\n",
        "    'progression': 1,\n",
        "    'convalescence': 2,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "uFjfMgZacWol"
      },
      "outputs": [],
      "source": [
        "class EmbeddedDataset(Dataset):\n",
        "    def __init__(self, pkl_file, minimum=0, n_samples=0):\n",
        "        self.data = pd.read_pickle(pkl_file)\n",
        "        self.minimum = minimum\n",
        "        self.downsample_labels()\n",
        "\n",
        "\n",
        "        unique_labels = pd.unique(self.data['severity'])\n",
        "        celltypes = pd.unique(self.data['celltype'])\n",
        "        # print(f\"unique cell types: {cell_types}\")\n",
        "        label_counts = {}\n",
        "        for label in self.data['severity']:\n",
        "            label_counts[label] = label_counts.get(label, 0) + 1\n",
        "        print(f\"unique labels: {unique_labels}\")\n",
        "        print(f\"label counts: {label_counts}\")\n",
        "        print(self.data.head())\n",
        "        if minimum != 0:\n",
        "            self.enforce_minimum()\n",
        "            if n_samples > 0:\n",
        "                self.enforce_number_of_samples(n_samples, self.minimum)\n",
        "                self.find_equal_distribution_with_number_of_samples(len(self.cell_types_list()), n_samples)\n",
        "            else:\n",
        "                print(\"Implement minimum again\")\n",
        "        elif n_samples > 0:\n",
        "            raise Exception(\"Illegal to specify number of samples without a minimum. How are we supposed to know what kind of distribution you want?\")\n",
        "\n",
        "    def cell_types_list(self):\n",
        "        return list(pd.unique(self.data['celltype']))\n",
        "    def distribution_cell_types(self):\n",
        "        return self.data['celltype'].value_counts()\n",
        "\n",
        "    def find_equal_distribution_with_number_of_samples(self, n_celltypes, n_samples):\n",
        "        \"\"\"Recursively finds a fitting distribution for the specified number of samples and minimum number of samples per celltype\"\"\"\n",
        "        if n_celltypes == 0:\n",
        "            raise Exception(f\"Could not find a valid data distribution for your combination of n_samples ({n_samples}) and minimum ({self.minimum}). \\n Consider decreasing one or both of them to get a valid distribution.\")\n",
        "        nth_celltype_count = self.distribution_cell_types().iloc[n_celltypes-1]\n",
        "        if(nth_celltype_count * n_celltypes > n_samples):\n",
        "            self.downsample_celltypes(n_celltypes, n_samples)\n",
        "        else:\n",
        "            self.find_equal_distribution_with_number_of_samples(n_celltypes-1, n_samples)\n",
        "\n",
        "    def enforce_minimum(self):\n",
        "        \"\"\"Cut all celltypes that does not have at least minimum samples\"\"\"\n",
        "        minimum_fulfilling_celltypes = self.distribution_cell_types()[self.distribution_cell_types() >= self.minimum].index\n",
        "        self.data = self.data[self.data['celltype'].isin(minimum_fulfilling_celltypes)]\n",
        "\n",
        "    def enforce_number_of_samples(self, n_samples, minimum):\n",
        "        \"\"\"Truncate the celltypes to a number of celltypes that suffice to reach the number of samples, while providing the minimum number of samples per celltype.\n",
        "        Always prefer celltypes with more counts over celltypes with less counts.\"\"\"\n",
        "        n_celltypes = min((n_samples // minimum), len(self.cell_types_list()))\n",
        "        self.data = self.data[self.data['celltype'].isin(self.distribution_cell_types().iloc[:n_celltypes].index)]\n",
        "\n",
        "    def downsample_labels(self):\n",
        "        \"\"\"Downsamples the data to achieve equal label distribution for severity within each cell type\"\"\"\n",
        "        cell_types = self.cell_types_list()\n",
        "        downsampled_data = pd.DataFrame()\n",
        "        for cell_type in cell_types:\n",
        "            cell_type_data = self.data[self.data['celltype'] == cell_type]\n",
        "            label_counts = cell_type_data['severity'].value_counts()\n",
        "            min_count = label_counts.min()\n",
        "            sampled_data = pd.DataFrame()\n",
        "            for label in label_counts.index:\n",
        "                label_data = cell_type_data[cell_type_data['severity'] == label]\n",
        "                sampled_label_data = label_data.sample(n=min_count, random_state=42)\n",
        "                sampled_data = pd.concat([sampled_data, sampled_label_data])\n",
        "            downsampled_data = pd.concat([downsampled_data, sampled_data])\n",
        "        self.data = downsampled_data\n",
        "\n",
        "    def downsample_celltypes(self, n_celltypes, n_samples):\n",
        "        \"\"\"Downsamples the data to achieve equal celltype distribution while providing the specified number of total samples\"\"\"\n",
        "        n = n_samples // n_celltypes\n",
        "        difference = n_samples - (n * n_celltypes)\n",
        "        valid_celltypes = self.distribution_cell_types().iloc[:n_celltypes].index\n",
        "        downsampled_data = pd.DataFrame()\n",
        "        for i, cell_type in enumerate(valid_celltypes):\n",
        "            cell_type_data = self.data[self.data['celltype'] == cell_type]\n",
        "            if i < difference:\n",
        "                sampled_data = cell_type_data.sample(n=n+1, random_state=42)\n",
        "            else:\n",
        "                sampled_data = cell_type_data.sample(n=n, random_state=42)\n",
        "            downsampled_data = pd.concat([downsampled_data, sampled_data])\n",
        "        self.data = downsampled_data\n",
        "        sample_sum = sum([x for x in self.data['celltype'].value_counts().values])\n",
        "        print(f\"\\nSum of samples: {sample_sum}\")\n",
        "        print(f\"Number of celltypes: {len(self.data['celltype'].value_counts())}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embeddings = torch.tensor(self.data.iloc[idx, 0], dtype=torch.float32)\n",
        "        label_state = torch.tensor(label_map.get(self.data.iloc[idx, 1]), dtype=torch.long)\n",
        "        label_severity = torch.tensor(label_map.get(self.data.iloc[idx, 3]), dtype=torch.long)\n",
        "        cell_type = self.data.iloc[idx, 2]\n",
        "\n",
        "        return embeddings, label_severity, label_state, cell_type"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Distribution"
      ],
      "metadata": {
        "id": "bEc-V2xFOH5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TOTAL_NUMBER_OF_SAMPLES = 60 * 1000\n",
        "MINIMUM_NUMBER_OF_SAMPLES_PER_CELLTYPE = 2000"
      ],
      "metadata": {
        "id": "uNb410WFJBHk"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Pac04bTdTov"
      },
      "outputs": [],
      "source": [
        "dataset = EmbeddedDataset(os.path.join(data_path, 'embedded_data_split_severity_celltype_progression_all.pkl'),MINIMUM_NUMBER_OF_SAMPLES_PER_CELLTYPE,TOTAL_NUMBER_OF_SAMPLES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFm_iWbOnmYI"
      },
      "outputs": [],
      "source": [
        "dataset.distribution_cell_types()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of different celltypes: {len(dataset.cell_types_list())}\")"
      ],
      "metadata": {
        "id": "LWoOOFwDM_0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# total_count = 0\n",
        "# for i, (celltype, count) in enumerate(dataset.distribution_cell_types().items()):\n",
        "#   total_count += count\n",
        "#   print(f\"{i:3} {celltype:28} {count} {total_count:7}\")"
      ],
      "metadata": {
        "id": "J_qcW-hWKWjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qILuHRdhu8j"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FduZTm4jhIH6"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFLzlS0kvAdY"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp8Ct-gYvhVZ"
      },
      "outputs": [],
      "source": [
        "for (embeddings, labels_severity,labels_state,_) in train_loader:\n",
        "    print(type(embeddings[0]))\n",
        "    print(type(labels_severity[0]))\n",
        "    print(type(embeddings[0][0]))\n",
        "    print(type(labels_severity[0].item()))\n",
        "    print(type(labels_state[0].item()))\n",
        "    print(labels_severity)\n",
        "    print(labels_state)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3sNfm7dvb2o"
      },
      "source": [
        "# Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q26P_D3UvdWu"
      },
      "outputs": [],
      "source": [
        "class Classifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(in_features=512, out_features=1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(in_features=1024, out_features=2048),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(in_features=2048, out_features=2048),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(in_features=2048, out_features=1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(in_features=1024, out_features=1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(in_features=1024, out_features=3),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll9Gc-pfmo5G"
      },
      "outputs": [],
      "source": [
        "# Training Loop\n",
        "def train(model, train_loader, num_epochs, optimizer, criterion):\n",
        "    model.to(device)  # Move model to device ONCE\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        dataloader = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for i, (embeddings, severity_labels, state_labels, celltype) in enumerate(dataloader):\n",
        "            embeddings = embeddings.to(device)\n",
        "            severity_labels = severity_labels.to(device).long()  # Labels: 0=Control, 1=Mild, 2=Severe\n",
        "            state_labels = state_labels.to(device).long()  # Labels: 0=Control, 1=Progression, 2=Convalescence\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(embeddings)  # Get raw logits\n",
        "\n",
        "            loss = criterion(output, severity_labels)  # Compute loss\n",
        "            loss.backward()  # Backpropagation\n",
        "            optimizer.step()  # Update weights\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            dataloader.set_postfix({\"Loss\": running_loss / (i + 1)})  # Show average loss per batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5gZvfHiyz3e"
      },
      "outputs": [],
      "source": [
        "# directory paths to google drive for model saving and retrieving\n",
        "\n",
        "directory_path = 'content/MyDrive/biomedicine/models/classifier'\n",
        "\n",
        "model_path = os.path.join(directory_path, f\"classifier_model_{TOTAL_NUMBER_OF_SAMPLES}_{TOTAL_NUMBER_OF_SAMPLES}.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Rz_PSGpsbgR"
      },
      "outputs": [],
      "source": [
        "# custom weights initialization\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 1/m.weight.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLqBjs4rzb72"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "\n",
        "model = Classifier().to(device)\n",
        "\n",
        "if LOAD_FROM_DRIVE:\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_path, weights_only=True))\n",
        "        print(\"Model loaded from drive\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model from drive: {e}\")\n",
        "# else:\n",
        "#     model.apply(weights_init)\n",
        "\n",
        "# set up loss function and optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rHnNd7840T_"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIrvs72E0Lus"
      },
      "outputs": [],
      "source": [
        "# start training loop\n",
        "if not LOAD_FROM_DRIVE:\n",
        "    train(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        num_epochs=NUM_EPOCHS,\n",
        "        optimizer=optimizer,\n",
        "        criterion=criterion\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSoHsDzv0QyT"
      },
      "outputs": [],
      "source": [
        "# save to drive\n",
        "\n",
        "if SAVE_TO_DRIVE:\n",
        "    try:\n",
        "        os.makedirs(directory_path)\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(\"Model saved to drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct0Lp8HK44gR"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nlqh9bfguQuM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    cell_type_results = defaultdict(lambda: {\"y_true\": [], \"y_pred\": []})  # Store results per cell type\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for embeddings, severity_labels, state_labels, cell_types in test_loader:\n",
        "            embeddings = embeddings.to(device)\n",
        "            severity_labels = severity_labels.to(device).long()  # Labels: 0=Control, 1=Mild, 2=Severe\n",
        "            state_labels = state_labels.to(device).long()  # Labels: 0=Control, 1=Progression, 2=Convalescence\n",
        "            labels = severity_labels # change depending on which output you want to evaluate\n",
        "\n",
        "            outputs = model(embeddings).squeeze()\n",
        "            preds = torch.argmax(outputs, dim=1)  # Convert probabilities to predicted labels\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Store results for each cell type\n",
        "            for i, cell in enumerate(cell_types):\n",
        "                cell_type_results[cell][\"y_true\"].append(labels[i].item())\n",
        "                cell_type_results[cell][\"y_pred\"].append(preds[i].item())\n",
        "\n",
        "    # Compute overall metrics\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
        "    recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
        "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    conf_matrix_normalized = conf_matrix.astype('float') / conf_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(6,5))\n",
        "    sns.heatmap(conf_matrix_normalized, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=[\"control\", \"mild/moderate\", \"severe/critical\"], yticklabels=[\"control\", \"mild/moderate\", \"severe/critical\"])\n",
        "    plt.xlabel(\"Predicted Labels\")\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # Compute metrics per cell type\n",
        "    cell_type_list, acc_list, f1_list, prec_list, rec_list = [], [], [], [], []\n",
        "    for cell, values in cell_type_results.items():\n",
        "        acc = accuracy_score(values[\"y_true\"], values[\"y_pred\"])\n",
        "        prec = precision_score(values[\"y_true\"], values[\"y_pred\"], average=\"macro\")\n",
        "        rec = recall_score(values[\"y_true\"], values[\"y_pred\"], average=\"macro\")\n",
        "        f1 = f1_score(values[\"y_true\"], values[\"y_pred\"], average=\"macro\")\n",
        "\n",
        "        cell_type_list.append(cell)\n",
        "        acc_list.append(acc)\n",
        "        f1_list.append(f1)\n",
        "        prec_list.append(prec)\n",
        "        rec_list.append(rec)\n",
        "\n",
        "    cell_type_results_df = pd.DataFrame({\n",
        "        \"Cell Type\": cell_type_list,\n",
        "        \"Accuracy\": acc_list,\n",
        "        \"F1-score\": f1_list,\n",
        "        \"Precision\": prec_list,\n",
        "        \"Recall\": rec_list\n",
        "    })\n",
        "\n",
        "    return accuracy, precision, recall, f1, cell_type_results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMpb5bMTolFE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "def compare_cell_type_distribution(dataset, train_loader, test_loader):\n",
        "    # Get total distribution from dataset (assuming it's a DataFrame)\n",
        "    total_distribution = dataset.distribution_cell_types()\n",
        "\n",
        "    # Ensure it's a DataFrame\n",
        "    if isinstance(total_distribution, pd.Series):\n",
        "        total_distribution = total_distribution.reset_index()\n",
        "        total_distribution.columns = [\"Cell Type\", \"Total Count\"]\n",
        "\n",
        "    # Initialize counters for train and test sets\n",
        "    train_counter = Counter()\n",
        "    test_counter = Counter()\n",
        "\n",
        "    # Count occurrences of each cell type in training set\n",
        "    for _, _, _, cell_types in train_loader:\n",
        "        train_counter.update(cell_types)\n",
        "\n",
        "    # Count occurrences of each cell type in test set\n",
        "    for _, _, _, cell_types in test_loader:\n",
        "        test_counter.update(cell_types)\n",
        "\n",
        "    # Convert counters to Pandas Series\n",
        "    train_series = pd.Series(train_counter, name=\"Train Count\")\n",
        "    test_series = pd.Series(test_counter, name=\"Test Count\")\n",
        "\n",
        "    # Create a DataFrame with all distributions\n",
        "    df = total_distribution.set_index(\"Cell Type\").join([train_series, test_series]).fillna(0)\n",
        "\n",
        "    # Convert counts to integers\n",
        "    df = df.astype(int)\n",
        "\n",
        "    # Print the table\n",
        "   # print(df)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FRwNSRzpfz6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "def analyze_correlation(dataset, train_loader, test_loader, model):\n",
        "    # Step 1: Get dataset distribution\n",
        "    total_distribution = dataset.distribution_cell_types()\n",
        "\n",
        "    # Ensure it's a DataFrame\n",
        "    if isinstance(total_distribution, pd.Series):\n",
        "        total_distribution = total_distribution.reset_index()\n",
        "        total_distribution.columns = [\"Cell Type\", \"Train Count\"]\n",
        "\n",
        "    # Step 2: Evaluate model performance per cell type\n",
        "    _, _, _, _, cell_results = evaluate(model, test_loader)\n",
        "    \"\"\"\n",
        "    # Step 3: Ensure cell_results is properly structured\n",
        "    if isinstance(cell_results, dict):\n",
        "        performance_data = []\n",
        "        for cell_type, metrics in cell_results.items():\n",
        "            if isinstance(metrics, dict):  # Ensure it's not a string or number\n",
        "                row = {\"Cell Type\": cell_type}\n",
        "                row.update(metrics)  # Add accuracy, F1-score, etc.\n",
        "                performance_data.append(row)\n",
        "        performance_df = pd.DataFrame(performance_data)\n",
        "    else:\n",
        "        raise ValueError(\"cell_results is not a valid dictionary.\")\n",
        "    \"\"\"\n",
        "    # Step 4: Merge both DataFrames\n",
        "    merged_df = total_distribution.merge(cell_results, on=\"Cell Type\", how=\"inner\")\n",
        "\n",
        "    # Step 5: Compute correlation\n",
        "    correlation_results = {\n",
        "        \"Accuracy\": pearsonr(merged_df[\"Train Count\"], merged_df[\"Accuracy\"])[0],\n",
        "        \"F1-score\": pearsonr(merged_df[\"Train Count\"], merged_df[\"F1-score\"])[0]\n",
        "    }\n",
        "\n",
        "    # Step 6: Visualization\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Scatter Plot: Frequency vs Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.scatterplot(x=merged_df[\"Train Count\"], y=merged_df[\"Accuracy\"])\n",
        "    plt.xlabel(\"Train Count of Cell Type\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(f\"Correlation: {correlation_results['Accuracy']:.4f}\")\n",
        "\n",
        "    # Scatter Plot: Frequency vs F1-score\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.scatterplot(x=merged_df[\"Train Count\"], y=merged_df[\"F1-score\"])\n",
        "    plt.xlabel(\"Train Count of Cell Type\")\n",
        "    plt.ylabel(\"F1-score\")\n",
        "    plt.title(f\"Correlation: {correlation_results['F1-score']:.4f}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print correlation results\n",
        "    print(\"Correlation between Cell Type Frequency and Performance:\")\n",
        "    print(f\"  Accuracy Correlation: {correlation_results['Accuracy']:.4f}\")\n",
        "    print(f\"  F1-score Correlation: {correlation_results['F1-score']:.4f}\")\n",
        "\n",
        "    return merged_df, correlation_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mM1EI22Pph6e"
      },
      "outputs": [],
      "source": [
        "# df_analysis, correlation = analyze_correlation(dataset, train_loader, test_loader, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results"
      ],
      "metadata": {
        "id": "leO23WriOVRt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeTbZlLv5QRC"
      },
      "outputs": [],
      "source": [
        "evaluate(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFYmCwHtomDz"
      },
      "outputs": [],
      "source": [
        "compare_cell_type_distribution(dataset, train_loader, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOqw2mYe5SAQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}