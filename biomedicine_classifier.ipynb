{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "J-nJiaMqar7p"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch import nn, optim\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2YIikC3b3rb",
        "outputId": "d177e801-8913-4a81-9534-1121af1ea4e4"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device is: {device}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IGJ2xusxekd",
        "outputId": "222b0dd8-bee1-4aba-9916-92a4b3068b0f"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device is: cuda\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/biomedicine/embedded_data/'"
      ],
      "metadata": {
        "id": "du_NFI00cDnx"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyper- and Controlparameter"
      ],
      "metadata": {
        "id": "Kn6-uMo-vNY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Controlparameter\n",
        "\n",
        "LOAD_FROM_DRIVE = False\n",
        "SAVE_TO_DRIVE = False"
      ],
      "metadata": {
        "id": "k0yW-SUWzh1m"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPERPARAMETER\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.0001\n",
        "NUM_EPOCHS = 10"
      ],
      "metadata": {
        "id": "ZpPHuDWPvGIP"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Dataset & Data Preparation"
      ],
      "metadata": {
        "id": "Lu8pwzyWvQAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# label_map = {\n",
        "#     'mild/moderate': 0,\n",
        "#     'severe/critical': 1,\n",
        "# }"
      ],
      "metadata": {
        "id": "8fclFGWJ23U1"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\n",
        "    'control': 0,\n",
        "    'mild/moderate': 1,\n",
        "    'severe/critical': 1,\n",
        "}"
      ],
      "metadata": {
        "id": "erJSlcIOrgTz"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddedDataset(Dataset):\n",
        "    def __init__(self, pkl_file):\n",
        "        self.data = pd.read_pickle(pkl_file)\n",
        "        unique_labels = pd.unique(self.data['severity'])\n",
        "        label_counts = {}\n",
        "        for label in self.data['severity']:\n",
        "            label_counts[label] = label_counts.get(label, 0) + 1\n",
        "        print(f\"unique labels: {unique_labels}\")\n",
        "        print(f\"label counts: {label_counts}\")\n",
        "        print(self.data.head())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        embeddings = torch.tensor(self.data.iloc[idx, 0], dtype=torch.float32)\n",
        "        label = torch.tensor(label_map.get(self.data.iloc[idx, 1]), dtype=torch.float32)\n",
        "        return embeddings, label"
      ],
      "metadata": {
        "id": "uFjfMgZacWol"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = EmbeddedDataset(os.path.join(data_path, 'embedded_data_split0_5.pkl'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Pac04bTdTov",
        "outputId": "15dfdfdf-9218-4c37-fe74-da1fa7fc8350"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique labels: ['mild/moderate' 'severe/critical' 'control']\n",
            "label counts: {'mild/moderate': 128980, 'severe/critical': 83187, 'control': 80373}\n",
            "                                          embeddings       severity\n",
            "0  [0.026112404, -0.021238996, 0.0003707884, -0.0...  mild/moderate\n",
            "1  [0.03304511, -0.013378126, 0.008212402, -0.005...  mild/moderate\n",
            "2  [0.020730188, -0.019310804, 0.00035625693, -0....  mild/moderate\n",
            "3  [0.028404342, -0.018271472, 0.006720709, 0.000...  mild/moderate\n",
            "4  [0.02587356, -0.014935524, -0.012807032, -0.00...  mild/moderate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
      ],
      "metadata": {
        "id": "FduZTm4jhIH6"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "LFLzlS0kvAdY"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for (embeddings, labels) in train_loader:\n",
        "    print(type(embeddings[0]))\n",
        "    print(type(labels[0]))\n",
        "    print(type(embeddings[0][0]))\n",
        "    print(type(labels[0].item()))\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp8Ct-gYvhVZ",
        "outputId": "dfee8948-ebb6-4b76-ae5a-25e6dcba0064"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "<class 'float'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Architecture\n"
      ],
      "metadata": {
        "id": "i3sNfm7dvb2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(in_features=512, out_features=1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(in_features=1024, out_features=2048),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(in_features=2048, out_features=2048),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(in_features=2048, out_features=1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(in_features=1024, out_features=1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(in_features=1024, out_features=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x).squeeze()"
      ],
      "metadata": {
        "id": "q26P_D3UvdWu"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainings loop\n",
        "\n",
        "def train(model, train_loader, num_epocs, optimizer, criterion):\n",
        "    for epoch in range(num_epocs):\n",
        "        model.to(device).train()\n",
        "        running_loss = 0.0\n",
        "        dataloader = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epocs}\")\n",
        "        for i, (embeddings, label) in enumerate(dataloader):\n",
        "            embeddings, label = embeddings.to(device), label.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(embeddings)\n",
        "            loss = criterion(output, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            dataloader.set_postfix({\"loss\":running_loss/(i+1)})"
      ],
      "metadata": {
        "id": "i9nyFq5TyIYn"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# directory paths to google drive for model saving and retrieving\n",
        "\n",
        "directory_path = 'content/MyDrive/biomedicine/models/classifier'\n",
        "\n",
        "binary_classifier_model_path = os.path.join(directory_path, 'binary_classifier.pth')"
      ],
      "metadata": {
        "id": "f5gZvfHiyz3e"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom weights initialization\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 1/m.weight.shape[1])"
      ],
      "metadata": {
        "id": "-Rz_PSGpsbgR"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "\n",
        "classifier_model = Classifier().to(device)\n",
        "\n",
        "if LOAD_FROM_DRIVE:\n",
        "    try:\n",
        "        classifier_model.load_state_dict(torch.load(binary_classifier_model_path, weights_only=True))\n",
        "        print(\"Model loaded from drive\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model from drive: {e}\")\n",
        "# else:\n",
        "#     classifier_model.apply(weights_init)\n",
        "\n",
        "# set up loss function and optimizer\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(classifier_model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "zLqBjs4rzb72"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "7rHnNd7840T_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start training loop\n",
        "if not LOAD_FROM_DRIVE:\n",
        "    train(\n",
        "        model=classifier_model,\n",
        "        train_loader=train_loader,\n",
        "        num_epocs=NUM_EPOCHS,\n",
        "        optimizer=optimizer,\n",
        "        criterion=criterion\n",
        "    )"
      ],
      "metadata": {
        "id": "CIrvs72E0Lus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d134f05-18a4-49f1-8bac-8dddb3e2bb22"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 1829/1829 [00:32<00:00, 55.68it/s, loss=0.274]\n",
            "Epoch 2/10: 100%|██████████| 1829/1829 [00:31<00:00, 57.20it/s, loss=0.178]\n",
            "Epoch 3/10: 100%|██████████| 1829/1829 [00:32<00:00, 56.05it/s, loss=0.15]\n",
            "Epoch 4/10: 100%|██████████| 1829/1829 [00:31<00:00, 57.35it/s, loss=0.133]\n",
            "Epoch 5/10: 100%|██████████| 1829/1829 [00:32<00:00, 56.13it/s, loss=0.122]\n",
            "Epoch 6/10: 100%|██████████| 1829/1829 [00:32<00:00, 56.86it/s, loss=0.115]\n",
            "Epoch 7/10: 100%|██████████| 1829/1829 [00:32<00:00, 56.47it/s, loss=0.108]\n",
            "Epoch 8/10: 100%|██████████| 1829/1829 [00:31<00:00, 57.19it/s, loss=0.103]\n",
            "Epoch 9/10: 100%|██████████| 1829/1829 [00:32<00:00, 56.85it/s, loss=0.0983]\n",
            "Epoch 10/10: 100%|██████████| 1829/1829 [00:32<00:00, 57.16it/s, loss=0.0958]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save to drive\n",
        "\n",
        "if SAVE_TO_DRIVE:\n",
        "    try:\n",
        "        os.makedirs(directory_path)\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "\n",
        "    torch.save(classifier_model.state_dict(), binary_classifier_model_path)\n",
        "    print(\"Model saved to drive\")"
      ],
      "metadata": {
        "id": "cSoHsDzv0QyT"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "Ct0Lp8HK44gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for embeddings, label in test_loader:\n",
        "            embeddings, label = embeddings.to(device), label.to(device)\n",
        "            outputs = [0 if i < 0.5 else 1 for i in model(embeddings)]\n",
        "            for i in range (len(outputs)):\n",
        "                if outputs[i] == label[i]:\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "    print(f\"Accuracy: {(correct*100/total):.2f} %\")"
      ],
      "metadata": {
        "id": "BtfgrzFi46Oy"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(classifier_model, test_loader)"
      ],
      "metadata": {
        "id": "DeTbZlLv5QRC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1eb9cb20-e130-4639-c50d-9fb3cb075f31"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 95.95 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wOqw2mYe5SAQ"
      },
      "execution_count": 94,
      "outputs": []
    }
  ]
}